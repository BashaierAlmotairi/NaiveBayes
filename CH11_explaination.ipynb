{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes classification\n",
    "Let's get right into it! Let's begin with Naïve Bayes classification. This machine\n",
    "learning model relies heavily on results from previous chapters, specifically with\n",
    "Bayes theorem:\n",
    "\n",
    "$$\n",
    "P(H~|~{\\rm D}) = \\frac{P({\\rm D}~|~H)P(H)}{P({\\rm D})}\n",
    "$$\n",
    "\n",
    "\n",
    "Let's look a little closer at the specific features of this formula: \n",
    "\n",
    "- P(H) is the probability of the hypothesis before we observe the data,\n",
    "called the prior probability, or just **(prior)**\n",
    "- P(H|D) is what we want to compute, the probability of the hypothesis\n",
    "after we observe the data, called the **(posterior)**\n",
    "- P(D|H) is the probability of the data under the given hypothesis, called the\n",
    "**(likelihood)**\n",
    "- P(D) is the probability of the data under any hypothesis, called the\n",
    "**(normalizing constant)**\n",
    "\n",
    "Naïve Bayes classification is a classification model, and therefore a supervised model.\n",
    "Given this, what kind of data do we need?\n",
    "- Labeled data\n",
    "- Unlabeled data\n",
    "\n",
    "If you answered labeled data then you're well on your way to becoming a data\n",
    "scientist!\n",
    "\n",
    "Suppose we have a data set with n features, (x1, x2, …, xn) and a class label C. For\n",
    "example let's take some data involving spam text classification. Our data would\n",
    "consist of rows of individual text samples and columns of both our features and our\n",
    "class labels. Our features would be words and phrases that are contained within the\n",
    "text samples and our class labels are simply **spam** or **not spam**. In this scenario, I will\n",
    "replace the class not spam with the easier to say word, **ham**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thats the way u feel. Thats the way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>Is that seriously how you spell his name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>I‘m going to try for 2 months ha ha only joking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ham</td>\n",
       "      <td>So ü pay first lar... Then when is da stock co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aft i finish my lunch then i go str down lor. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ffffffffff. Alright no way I can meet up with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ham</td>\n",
       "      <td>Just forced myself to eat a slice. I'm really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ham</td>\n",
       "      <td>Lol your always so convincing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ham</td>\n",
       "      <td>Did you catch the bus ? Are you frying an egg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm back &amp;amp; we're packing the car now, I'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ahhh. Work. I vaguely remember that! What does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>ham</td>\n",
       "      <td>Armand says get your ass over to epsilon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>ham</td>\n",
       "      <td>U still havent got urself a jacket ah?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm taking derek &amp;amp; taylor to walmart, if I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hi its in durban are you still on this number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ic. There are a lotta childporn cars then.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>ham</td>\n",
       "      <td>No, I was trying it all weekend ;V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>ham</td>\n",
       "      <td>You know, wot people wear. T shirts, jumpers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>ham</td>\n",
       "      <td>Cool, what time you think you can get here?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wen did you get so spiritual and deep. That's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have a safe trip to Nigeria. Wish you happines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5553</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hahaha..use your brain dear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>ham</td>\n",
       "      <td>Well keep in mind I've only got enough gas for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeh. Indians was nice. Tho it did kane me off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes i have. So that's why u texted. Pshew...mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>ham</td>\n",
       "      <td>No. I meant the calculation is the same. That ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>ham</td>\n",
       "      <td>if you aren't here in the next  &amp;lt;#&amp;gt;  hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>ham</td>\n",
       "      <td>Anything lor. Juz both of us lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>ham</td>\n",
       "      <td>Get me out of this dump heap. My mom decided t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lor... Sony ericsson salesman... I ask shuh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Why don't you wait 'til at least wednesday to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>Huh y lei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                                msg\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6      ham  Even my brother is not like to speak with me. ...\n",
       "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8     spam  WINNER!! As a valued network customer you have...\n",
       "9     spam  Had your mobile 11 months or more? U R entitle...\n",
       "10     ham  I'm gonna be home soon and i don't want to tal...\n",
       "11    spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "12    spam  URGENT! You have won a 1 week FREE membership ...\n",
       "13     ham  I've been searching for the right words to tha...\n",
       "14     ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "15    spam  XXXMobileMovieClub: To use your credit, click ...\n",
       "16     ham                         Oh k...i'm watching here:)\n",
       "17     ham  Eh u remember how 2 spell his name... Yes i di...\n",
       "18     ham  Fine if thats the way u feel. Thats the way ...\n",
       "19    spam  England v Macedonia - dont miss the goals/team...\n",
       "20     ham          Is that seriously how you spell his name?\n",
       "21     ham    I‘m going to try for 2 months ha ha only joking\n",
       "22     ham  So ü pay first lar... Then when is da stock co...\n",
       "23     ham  Aft i finish my lunch then i go str down lor. ...\n",
       "24     ham  Ffffffffff. Alright no way I can meet up with ...\n",
       "25     ham  Just forced myself to eat a slice. I'm really ...\n",
       "26     ham                     Lol your always so convincing.\n",
       "27     ham  Did you catch the bus ? Are you frying an egg ...\n",
       "28     ham  I'm back &amp; we're packing the car now, I'll...\n",
       "29     ham  Ahhh. Work. I vaguely remember that! What does...\n",
       "...    ...                                                ...\n",
       "5542   ham           Armand says get your ass over to epsilon\n",
       "5543   ham             U still havent got urself a jacket ah?\n",
       "5544   ham  I'm taking derek &amp; taylor to walmart, if I...\n",
       "5545   ham      Hi its in durban are you still on this number\n",
       "5546   ham         Ic. There are a lotta childporn cars then.\n",
       "5547  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5548   ham                 No, I was trying it all weekend ;V\n",
       "5549   ham  You know, wot people wear. T shirts, jumpers, ...\n",
       "5550   ham        Cool, what time you think you can get here?\n",
       "5551   ham  Wen did you get so spiritual and deep. That's ...\n",
       "5552   ham  Have a safe trip to Nigeria. Wish you happines...\n",
       "5553   ham                        Hahaha..use your brain dear\n",
       "5554   ham  Well keep in mind I've only got enough gas for...\n",
       "5555   ham  Yeh. Indians was nice. Tho it did kane me off ...\n",
       "5556   ham  Yes i have. So that's why u texted. Pshew...mi...\n",
       "5557   ham  No. I meant the calculation is the same. That ...\n",
       "5558   ham                             Sorry, I'll call later\n",
       "5559   ham  if you aren't here in the next  &lt;#&gt;  hou...\n",
       "5560   ham                  Anything lor. Juz both of us lor.\n",
       "5561   ham  Get me out of this dump heap. My mom decided t...\n",
       "5562   ham  Ok lor... Sony ericsson salesman... I ask shuh...\n",
       "5563   ham                                Ard 6 like dat lor.\n",
       "5564   ham  Why don't you wait 'til at least wednesday to ...\n",
       "5565   ham                                       Huh y lei...\n",
       "5566  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_table('https://raw.githubusercontent.com/sinanuozdemir/sfdat22/master/data/sms.tsv',\n",
    "sep='\\t', header=None, names=['label', 'msg'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some preliminary statistics to see what we are dealing with. Let's see the\n",
    "difference in the number of ham and spam messages at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x95bf048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have WAY more ham messages than we do spam. Because this is a\n",
    "classification problem, it will be very useful to know our null accuracy rate which is\n",
    "the percentage chance of predicting a single row correctly if we keep guessing the\n",
    "most common class, ham:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts() / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we blindly guessed ham we would be correct about **87%** of the time, but we can\n",
    "do better than that. If we have a set of classes, C, and a features xi, then we can use\n",
    "Bayes theorem to predict the probability that a single row belongs to class C using\n",
    "the following formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(classC~|~{\\rm {Xi} }) = \\frac{P({\\rm Xi}~|~classC)P(classC)}{P({\\rm Xi})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this formula in a little more detail:\n",
    "- P(class C | {xi}): The posterior probability is the probability that the row\n",
    "belongs to class C given the features {xi}.\n",
    "- P({xi} | class C): This is the likelihood that we would observe these features\n",
    "given that the row was in class C.\n",
    "- P(class C): This is the prior probability. It is the probability that the data point\n",
    "belongs to class C before we see any data.\n",
    "- P({xi}): This is our normalization constant. \n",
    "\n",
    "For example, imagine we have an e-mail with three words: send cash now. We'll\n",
    "use Naïve Bayes to classify the e-mail as either being **spam or ham**:\n",
    "\n",
    "*P(spam| send cash now) = P(send cash now| spam)\u001c",
    " P(spam) / P(send cash now)*\n",
    "\n",
    "*P(ham| send cash now) = P(send cash now| ham)\u001c",
    " P(ham) / P(send cash now)*\n",
    "\n",
    "We are concerned with the difference of these two numbers. We can use the\n",
    "following criteria to classify any single text sample:\n",
    "- If P(spam | send cash now) is larger than P(ham | send cash now),\n",
    "then we will classify the text as spam\n",
    "- If P(ham | send cash now) is larger than P(spam | send cash now), then\n",
    "we will label the text as ham\n",
    "\n",
    "Because both equations have P (send money now) on the denominator, we can\n",
    "ignore them.\n",
    "So now we are concerned with the following:\n",
    "\n",
    "**P(send cash now| spam)\u001d",
    " P(spam) VS P(send cash now| ham)\u001d",
    " P(ham)**\n",
    "\n",
    "Let's figure out the numbers in this equation:\n",
    "- P(spam) = 0.134063\n",
    "- P(ham) = 0.865937\n",
    "- P(send cash now | spam)\n",
    "- P(send cash now | ham)\n",
    "\n",
    "The final two likelihoods might seem like they would not be so difficult to calculate.\n",
    "All we have to do is count the numbers of spam messages that include the phrase\n",
    "send money now and divide that by the total number of spam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.msg = df.msg.apply(lambda x:x.lower())\n",
    "# make all strings lower case so we can search easier\n",
    "\n",
    "df[df.msg.str.contains('send cash now')] .shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! There are none! There are literally 0 texts with the exact phrase send cash\n",
    "now. The hidden problem here is that this phrase is very specific and we can't assume\n",
    "that we will have enough data in the world to have seen this exact phrase many\n",
    "times before. Instead we can make a naïve assumption in our Bayes theorem. If we\n",
    "assume that the features (words) are conditionally independent (meaning that no\n",
    "word affects the existence of another word) then we can rewrite the formula:\n",
    "\n",
    "**P(send cash now| spam) = P(send | spam)\u001c",
    " P(cash | spam)\u001c",
    " P(now| spam)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-ede449ab591f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-ede449ab591f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print word, spams[spams.msg.str.contains(word)].shape[0] / float(spams.shape[0])revealing\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "spams = df[df.label == 'spam']\n",
    "for word in ['send', 'cash', 'now']:\n",
    "    print word, spams[spams.msg.str.contains(word)].shape[0] / float(spams.shape[0])revealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(send|spam) = 0.096\n",
    "- P(cash |spam) = 0.091\n",
    "- P(now|spam) = 0.280\n",
    "\n",
    "Meaning we can calculate the following:\n",
    "\n",
    "**P(send cash now| spam)\u001d",
    " P(spam)= (.096\u001d",
    ".091\u001d",
    ".280)\u001d",
    ".134 = 0.00032**\n",
    "\n",
    "Repeating the same procedure for ham gives us the following:\n",
    "\n",
    "- P(send|ham) = 0.03\n",
    "- P(cash|ham) = 0.003\n",
    "- P(now|ham) = 0.109\n",
    "\n",
    "**P(send cash now| ham)\u001d",
    " P(ham)= (.03\u001d",
    ".003\u001d",
    ".109)\u001d",
    ".865 = 0.0000084**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that these numbers are both very low is not as important as the fact that\n",
    "the spam probability is much larger than the ham calculation. If we calculate .00032\n",
    "/ .0000084 = 38.1 we see that the send cash now probability for spam is 38 times\n",
    "higher than for spam.\n",
    "\n",
    "Doing this means that we can classify send cash now as spam! Simple, right?\n",
    "\n",
    "Let's use Python to implement a Naïve Bayes classifier without having to do all of\n",
    "these calculations ourselves.\n",
    "\n",
    "First, let's revisit the count vectorizer in scikit-learn that turns text into numerical\n",
    "data for us. Let's assume that we will train on three documents (sentences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>44</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   44  cab  call  me  please  tonight  you\n",
       "0   0    0     1   0       0        1    1\n",
       "1   0    1     1   1       0        0    0\n",
       "2   1    0     1   1       2        0    0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple count vectorizer example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# start with a simple example\n",
    "train_simple = ['call you tonight','Call me a cab','please call me... PLEASE 44!']\n",
    "\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect = CountVectorizer()\n",
    "train_simple_dtm = vect.fit_transform(train_simple)\n",
    "pd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each row represents one of the three documents (sentences), each column\n",
    "represents one of the words present in the documents and each cell contains the\n",
    "number of times each word appears in each document.\n",
    "\n",
    "We can then use the count vectorizer to transform new incoming test documents to\n",
    "conform with our training set (the three sentences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>44</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   44  cab  call  me  please  tonight  you\n",
       "0   0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary, notice don't is missing)\n",
    "test_simple = [\"please don't call me\"]\n",
    "test_simple_dtm = vect.transform(test_simple)\n",
    "test_simple_dtm.toarray()\n",
    "pd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how in our test sentence we had a new word, namely don't. When we\n",
    "vectorized it, because we hadn't seen that word previously in our training data, the\n",
    "vectorizer simply ignored it. This is important, and incentivizes data scientists to\n",
    "obtain as much data as possible for their training sets.\n",
    "\n",
    "\n",
    "Now let's do this for our actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.msg, df.label, random_state=1)\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# learn vocabulary and create document-term matrix in a single step\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 55209 stored elements in compressed sparse row format.\n",
    "\n",
    "\n",
    "Note that the format is in a sparse matrix, meaning the matrix is so large and full of\n",
    "zeroes, there exists a special format to deal with objects such as this. Take a look at\n",
    "the number of columns.\n",
    "\n",
    "\n",
    "7,456 words!!\n",
    "\n",
    "This means that in our training set, there are 7,456 unique words to look at. We can\n",
    "now transform our test data to conform to our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix\n",
    "test_dtm = vect.transform(X_test)\n",
    "test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 17604 stored elements in compressed sparse row format.\n",
    "N\n",
    "ote that we have the same exact number of columns because it is conforming to our\n",
    "test set to be exactly the same vocabulary as before. No more, no less.\n",
    "\n",
    "Now let's build a Naïve Bayes model (similar to the linear regression process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MODEL BUILDING WITH NAIVE BAYES\n",
    "# train a Naive Bayes model using train_dtm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# import our model\n",
    "\n",
    "nb = MultinomialNB()\n",
    "# instantiate our model\n",
    "\n",
    "nb.fit(train_dtm, y_train)\n",
    "# fit it to our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the variable nb holds our fitted model. The training phase of the model\n",
    "involves computing the likelihood function, which is the conditional probability of\n",
    "each feature given each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'spam', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions on test data using test_dtm\n",
    "preds = nb.predict(test_dtm)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction phase of the model involves computing the posterior probability\n",
    "of each class given the observed features, and choosing the class with the highest\n",
    "probability.\n",
    "\n",
    "\n",
    "We will use sklearn's built-in accuracy and confusion matrix to look at how well our\n",
    "Naïve Bayes models are performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-3c2407e9dd05>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-3c2407e9dd05>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print metrics.accuracy_score(y_test, preds)\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# compare predictions to true labels\n",
    "from sklearn import metrics\n",
    "print metrics.accuracy_score(y_test, preds)\n",
    "print metrics.confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy == 0.988513998564\n",
    "\n",
    "confusion matrix ==\n",
    "[[1203 5]\n",
    "[ 11 174]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, our accuracy is great! Compared to our null accuracy which was 87%, 99%\n",
    "is a fantastic improvement.\n",
    "\n",
    "\n",
    "Now to our confusion matrix. From before, we know that each row represents\n",
    "actual values while columns represent predicted values so the top left value, 1,203,\n",
    "represents our true negatives. But what is negative and positive? We gave the model\n",
    "the strings spam and ham as our classes, not positive and negative.\n",
    "\n",
    "\n",
    "We can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype='<U4')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then line up the indices so that the 1,203 refers to true ham predictions and\n",
    "174 refers to true spam predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were also five false spam classifications, meaning that five messages were\n",
    "predicted as spam, but were actually ham, as well as 11 false ham classifications.\n",
    "\n",
    "In summary, Naïve Bayes classification uses Bayes theorem in order to fit posterior\n",
    "probabilities of classes so that data points are correctly labeled as belonging to the\n",
    "proper class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is an intersting clip to watch on YOUTUBE \n",
    "\n",
    "https://www.youtube.com/watch?v=CPqOCI0ahss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
